import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc, precision_score, recall_score, f1_score,
    precision_recall_curve
)
from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier

# Load dataset
train_data = pd.read_csv(r"C:\Users\SWEETHA\Downloads\archive (4)\train.csv"
)
test_data = pd.read_csv(r"C:\Users\SWEETHA\Downloads\archive (4)\test.csv")

# Fill missing values
train_data.fillna(0, inplace=True)
test_data.fillna(0, inplace=True)

# Features
selected_features = [
    'profile pic', 'nums/length username', 'fullname words', 'nums/length
fullname',
    'name==username', 'description length', 'external URL', 'private',
    '#posts', '#followers', '#follows'
]

X_train = train_data[selected_features]
y_train = train_data['fake']
X_test = test_data[selected_features]
y_test = test_data['fake']

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled,
y_train)

# Hyperparameter Grids
param_grid_rf = {'n_estimators': [100, 200], 'max_depth': [10, 20],
'min_samples_split': [2, 5]}
param_grid_lr = {'C': [0.1, 1, 10], 'solver': ['liblinear']}
param_grid_cb = {'iterations': [100, 200], 'learning_rate': [0.05, 0.1],
'depth': [3, 5]}
param_grid_svc = {'C': [1, 10], 'kernel': ['linear', 'rbf']}

# Models
models = {
    'RandomForest': GridSearchCV(RandomForestClassifier(random_state=42,
class_weight='balanced'), param_grid_rf, cv=3, n_jobs=-1),
    'LogisticRegression': GridSearchCV(LogisticRegression(max_iter=1000,
class_weight='balanced', random_state=42), param_grid_lr, cv=3, n_jobs=-1),
    'CatBoost': GridSearchCV(CatBoostClassifier(verbose=0, random_state=42,
allow_writing_files=False), param_grid_cb, cv=3, n_jobs=-1),
    'SVC': GridSearchCV(SVC(probability=True, class_weight='balanced',
random_state=42), param_grid_svc, cv=3, n_jobs=-1)
}

# Train and Evaluate
plt.figure(figsize=(8, 6))
for name, grid in models.items():
    print(f"\nTraining {name}...")
    grid.fit(X_train_resampled, y_train_resampled)
    best_model = grid.best_estimator_

    y_prob = best_model.predict_proba(X_test_scaled)[:, 1]
    threshold = 0.3
    y_pred = (y_prob >= threshold).astype(int)

    print(f"Best Params: {grid.best_params_}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred):.4f} | Recall:
{recall_score(y_test,
y_pred):.4f} | F1: {f1_score(y_test, y_pred):.4f}")
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # ROC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

    # Save model
    joblib.dump(best_model, f'{name}_model.pkl')

# Final ROC Plot
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.tight_layout()
plt.show()
plt.close()

# Precision-Recall Curve
plt.figure(figsize=(8, 6))
for name, grid in models.items():
    best_model = grid.best_estimator_
    y_prob = best_model.predict_proba(X_test_scaled)[:, 1]
    precision, recall, _ = precision_recall_curve(y_test, y_prob)
    plt.plot(recall, precision, label=name)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.tight_layout()
plt.show()
plt.close()

# Save Scaler
joblib.dump(scaler, 'scaler.pkl')

# Feature Importance (Random Forest)
rf_model = models['RandomForest'].best_estimator_
importances = rf_model.feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(selected_features, importances)
plt.xlabel("Importance")
plt.title("Feature Importance (Random Forest)")
plt.tight_layout()
plt.show()
plt.close()

