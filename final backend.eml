from flask import Flask, request, jsonify
from flask_cors import CORS
import joblib
from sentence_transformers import SentenceTransformer, util
import numpy as np

app = Flask(__name__)
CORS(app)

# Load ML models
models_dict = {
    'RandomForest': joblib.load(r"C:\Users\SWEETHA\OneDrive\Desktop\swetha
2023\final project\RandomForest_model.pkl"),
    'CatBoost': joblib.load(r"C:\Users\SWEETHA\OneDrive\Desktop\swetha 2023
\final project\CatBoost_model.pkl"),
    'LogisticRegression':
joblib.load(r"C:\Users\SWEETHA\OneDrive\Desktop\swetha
2023\final project\LogisticRegression_model.pkl"),
    'SVC': joblib.load(r"C:\Users\SWEETHA\OneDrive\Desktop\swetha 2023\final
project\SVC_model.pkl")
}

scaler = joblib.load(r"C:\Users\SWEETHA\OneDrive\Desktop\swetha 2023\final
project\scaler.pkl")
semantic_model = SentenceTransformer('all-MiniLM-L6-v2')

# Suspicious phrases for semantic bio analysis
suspicious_phrases = [
    "Follow 4 Follow", "Gain followers fast", "Get rich quick", "Limited
time offer",
    "Earn money fast", "Boost your account now", "Amazing discount on likes"
,
    "100% guarantee on boosting your account", "Guaranteed results", "Exclusive
deal"
]

# Semantic similarity for bio check
def check_suspicious_bio(bio):
    bio_embedding = semantic_model.encode(bio, convert_to_tensor=True)
    for phrase in suspicious_phrases:
        phrase_embedding = semantic_model.encode(phrase, convert_to_tensor=
True)
        similarity = util.pytorch_cos_sim(bio_embedding,
phrase_embedding).item()
        if similarity > 0.7:
            return 1
    return 0

# Extract 11 features from 9 frontend inputs
def extract_features(data):
    username = data.get('username', '')
    fullname = data.get('fullname', '')
    bio = data.get('bio', '')

    nums_len_username = sum(1 for c in username if c.isdigit()) / len(username)
if username else 0
    fullname_words = len(fullname.strip().split())
    nums_len_fullname = sum(1 for c in fullname if c.isdigit()) / len(fullname)
if fullname else 0
    name_equals_username = int(username.strip().lower() ==
fullname.strip().lower())
    bio_length = len(bio)

    profile_pic = int(data.get('profile_pic', False))
    external_url = int(data.get('external_url', False))
    is_private = int(data.get('is_private', False))
    num_posts = int(data.get('num_posts', 0))
    num_followers = int(data.get('num_followers', 0))
    num_following = int(data.get('num_following', 0))

    features = [
        profile_pic,
        nums_len_username,
        fullname_words,
        nums_len_fullname,
        name_equals_username,
        bio_length,
        external_url,
        is_private,
        num_posts,
        num_followers,
        num_following
    ]

    return features, bio

# Main API route
@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        detailed = data.get('detailed', False)

        features, bio = extract_features(data)
        features_scaled = scaler.transform([features])
        bio_suspicion_score = check_suspicious_bio(bio)

        all_predictions = {}

        # Debugging: Print extracted features and suspicion score
        print(f"Features: {features}")
        print(f"Suspicion Score for Bio: {bio_suspicion_score}")

        # Iterate through all models to predict
        for model_name, model in models_dict.items():
            prob_fake = model.predict_proba(features_scaled)[0][1]
            weighted_score = (0.7 * prob_fake) + (0.3 * bio_suspicion_score)

            # Debugging: Print prediction and prob_fake values
            print(f"Model: {model_name}, prob_fake: {prob_fake},
weighted_score: {weighted_score}")

            # Assign prediction based on confidence score
            if weighted_score >= 0.60:
                label = "Fake Account"
            else:
                label = "Genuine Account"

            # Add model prediction and confidence score to the dictionary
            all_predictions[model_name] = {
                "prediction": label,
                "confidence_score": round(weighted_score, 4)
            }

        # Debugging: Print all predictions before selecting best model
        print(f"All Predictions: {all_predictions}")

        # Get the best model based on highest confidence score
        best_model = max(all_predictions, key=lambda m: all_predictions[m][
"confidence_score"])
        best_result = all_predictions[best_model]

        response = {
            "best_model_used": best_model,
            "prediction": best_result["prediction"],
            "confidence_score": best_result["confidence_score"]
        }

        # If detailed flag is True, include all model predictions
        if detailed:
            response["all_model_predictions"] = all_predictions

        # Debugging: Print response before returning
        print(f"Response: {response}")

        return jsonify(response)

    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)  # For HTTPS locally (if needed)





lor:rgb(106,153,85)"># For HTTPS locally (if needed)</span></div><br><br></=
div></div>


--000000000000da2b7c06360b0f68--
